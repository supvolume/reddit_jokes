{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caring-chosen",
   "metadata": {},
   "source": [
    "# Trial for finding frequently mentioned person and organisation in jokes (using NLTK package)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-stake",
   "metadata": {},
   "source": [
    "I notice that most of the jokes usually involve “a man”, “a wife”, “a doctor”, “a priest”, etc. This makes me wonder what is the most appear character (include organisation) in the jokes.\n",
    "So, the goal of this project is to find the frequently mentioned character in the jokes. Another goal is to find the difference between safe for work (SFW) and not safe for work (NSFW) jokes.\n",
    "I selected data from subreddit /r/Jokes because it has NSFW tag which can be used as the identifier between two types of jokes.\n",
    "\n",
    "The jokes scaping code and data can be found [here](https://github.com/supvolume/top_reddit_jokes/tree/main/scape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cubic-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.chunk import tree2conlltags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "royal-edward",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>over_18</th>\n",
       "      <th>url</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Calm down about the Net Neutrality thing...</td>\n",
       "      <td>Victorinox2</td>\n",
       "      <td>136356</td>\n",
       "      <td>1612</td>\n",
       "      <td>1511324127</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/Jokes/comments/7ekt23...</td>\n",
       "      <td>Paying additional money to access certain site...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V</td>\n",
       "      <td>MadGo</td>\n",
       "      <td>106424</td>\n",
       "      <td>1361</td>\n",
       "      <td>1499306465</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/Jokes/comments/6lfqep/v/</td>\n",
       "      <td>V\\n\\n*Edit: seems like the ctrl key on my keyb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If your surprised that Jeffrey Epstein commite...</td>\n",
       "      <td>williseeyoutonight</td>\n",
       "      <td>103655</td>\n",
       "      <td>2422</td>\n",
       "      <td>1565478205</td>\n",
       "      <td>True</td>\n",
       "      <td>https://www.reddit.com/r/Jokes/comments/coj45m...</td>\n",
       "      <td>Imagine how surprised he must have been.\\n\\nEd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A new Navy recruit has his first day on the su...</td>\n",
       "      <td>Ckarini</td>\n",
       "      <td>98262</td>\n",
       "      <td>765</td>\n",
       "      <td>1539035627</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/Jokes/comments/9mf1cz...</td>\n",
       "      <td>He speaks with the officer, who assigns him hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The only two white actors in Black Panther are...</td>\n",
       "      <td>bananastanding</td>\n",
       "      <td>94941</td>\n",
       "      <td>1379</td>\n",
       "      <td>1518396184</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/Jokes/comments/7wtsuf...</td>\n",
       "      <td>They're the Tolkien white guys.\\n\\nEdit: Appar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title              author  \\\n",
       "1        Calm down about the Net Neutrality thing...         Victorinox2   \n",
       "2                                                  V               MadGo   \n",
       "3  If your surprised that Jeffrey Epstein commite...  williseeyoutonight   \n",
       "4  A new Navy recruit has his first day on the su...             Ckarini   \n",
       "5  The only two white actors in Black Panther are...      bananastanding   \n",
       "\n",
       "    score  comms_num     created  over_18  \\\n",
       "1  136356       1612  1511324127    False   \n",
       "2  106424       1361  1499306465    False   \n",
       "3  103655       2422  1565478205     True   \n",
       "4   98262        765  1539035627    False   \n",
       "5   94941       1379  1518396184    False   \n",
       "\n",
       "                                                 url  \\\n",
       "1  https://www.reddit.com/r/Jokes/comments/7ekt23...   \n",
       "2  https://www.reddit.com/r/Jokes/comments/6lfqep/v/   \n",
       "3  https://www.reddit.com/r/Jokes/comments/coj45m...   \n",
       "4  https://www.reddit.com/r/Jokes/comments/9mf1cz...   \n",
       "5  https://www.reddit.com/r/Jokes/comments/7wtsuf...   \n",
       "\n",
       "                                                body  \n",
       "1  Paying additional money to access certain site...  \n",
       "2  V\\n\\n*Edit: seems like the ctrl key on my keyb...  \n",
       "3  Imagine how surprised he must have been.\\n\\nEd...  \n",
       "4  He speaks with the officer, who assigns him hi...  \n",
       "5  They're the Tolkien white guys.\\n\\nEdit: Appar...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file\n",
    "joke_df = pd.read_json(\"top_reddit_joke.json\")\n",
    "\n",
    "# Remove first row because it not a joke post\n",
    "joke_df = joke_df.drop(0)\n",
    "joke_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-product",
   "metadata": {},
   "source": [
    "## The steps for obtaining character in jokes\n",
    "\n",
    "1. Tokenization to split sentences into words by using `word_tokenize`\n",
    "2. Tagging to identify the part of speech with `pos_tag` \n",
    "3. Get the noun \n",
    "4. Lemmatisation\n",
    "5. Separate between SFW and NSFW jokes\n",
    "6. Count the nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-welcome",
   "metadata": {},
   "source": [
    "### 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "advised-economy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>title_t</th>\n",
       "      <th>body_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Calm down about the Net Neutrality thing...</td>\n",
       "      <td>Paying additional money to access certain site...</td>\n",
       "      <td>[Calm, down, about, the, Net, Neutrality, thin...</td>\n",
       "      <td>[Paying, additional, money, to, access, certai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V</td>\n",
       "      <td>V\\n\\n*Edit: seems like the ctrl key on my keyb...</td>\n",
       "      <td>[V]</td>\n",
       "      <td>[V, *, Edit, :, seems, like, the, ctrl, key, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If your surprised that Jeffrey Epstein commite...</td>\n",
       "      <td>Imagine how surprised he must have been.\\n\\nEd...</td>\n",
       "      <td>[If, your, surprised, that, Jeffrey, Epstein, ...</td>\n",
       "      <td>[Imagine, how, surprised, he, must, have, been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A new Navy recruit has his first day on the su...</td>\n",
       "      <td>He speaks with the officer, who assigns him hi...</td>\n",
       "      <td>[A, new, Navy, recruit, has, his, first, day, ...</td>\n",
       "      <td>[He, speaks, with, the, officer, ,, who, assig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The only two white actors in Black Panther are...</td>\n",
       "      <td>They're the Tolkien white guys.\\n\\nEdit: Appar...</td>\n",
       "      <td>[The, only, two, white, actors, in, Black, Pan...</td>\n",
       "      <td>[They, 're, the, Tolkien, white, guys, ., Edit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "1        Calm down about the Net Neutrality thing...   \n",
       "2                                                  V   \n",
       "3  If your surprised that Jeffrey Epstein commite...   \n",
       "4  A new Navy recruit has his first day on the su...   \n",
       "5  The only two white actors in Black Panther are...   \n",
       "\n",
       "                                                body  \\\n",
       "1  Paying additional money to access certain site...   \n",
       "2  V\\n\\n*Edit: seems like the ctrl key on my keyb...   \n",
       "3  Imagine how surprised he must have been.\\n\\nEd...   \n",
       "4  He speaks with the officer, who assigns him hi...   \n",
       "5  They're the Tolkien white guys.\\n\\nEdit: Appar...   \n",
       "\n",
       "                                             title_t  \\\n",
       "1  [Calm, down, about, the, Net, Neutrality, thin...   \n",
       "2                                                [V]   \n",
       "3  [If, your, surprised, that, Jeffrey, Epstein, ...   \n",
       "4  [A, new, Navy, recruit, has, his, first, day, ...   \n",
       "5  [The, only, two, white, actors, in, Black, Pan...   \n",
       "\n",
       "                                              body_t  \n",
       "1  [Paying, additional, money, to, access, certai...  \n",
       "2  [V, *, Edit, :, seems, like, the, ctrl, key, o...  \n",
       "3  [Imagine, how, surprised, he, must, have, been...  \n",
       "4  [He, speaks, with, the, officer, ,, who, assig...  \n",
       "5  [They, 're, the, Tolkien, white, guys, ., Edit...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize\n",
    "joke_df[\"title_t\"] = joke_df[\"title\"].apply(word_tokenize)\n",
    "joke_df[\"body_t\"] = joke_df[\"body\"].apply(word_tokenize)\n",
    "\n",
    "select_col = [\"title\", \"body\", \"title_t\", \"body_t\"]\n",
    "joke_df[select_col].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-chinese",
   "metadata": {},
   "source": [
    "### 2. Tagging\n",
    "\n",
    "This step keeps the title and body separated because the context might help in identifying the tag.\n",
    "Note that capitalisation affects tagging step, thus noun will be changed to lower case after tagging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prescribed-popularity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>title_tag</th>\n",
       "      <th>body_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Calm down about the Net Neutrality thing...</td>\n",
       "      <td>Paying additional money to access certain site...</td>\n",
       "      <td>[(Calm, NNP), (down, RB), (about, IN), (the, D...</td>\n",
       "      <td>[(Paying, VBG), (additional, JJ), (money, NN),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V</td>\n",
       "      <td>V\\n\\n*Edit: seems like the ctrl key on my keyb...</td>\n",
       "      <td>[(V, NN)]</td>\n",
       "      <td>[(V, NNP), (*, NNP), (Edit, NNP), (:, :), (see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If your surprised that Jeffrey Epstein commite...</td>\n",
       "      <td>Imagine how surprised he must have been.\\n\\nEd...</td>\n",
       "      <td>[(If, IN), (your, PRP$), (surprised, VBD), (th...</td>\n",
       "      <td>[(Imagine, NNP), (how, WRB), (surprised, VBD),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A new Navy recruit has his first day on the su...</td>\n",
       "      <td>He speaks with the officer, who assigns him hi...</td>\n",
       "      <td>[(A, DT), (new, JJ), (Navy, NNP), (recruit, NN...</td>\n",
       "      <td>[(He, PRP), (speaks, VBZ), (with, IN), (the, D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The only two white actors in Black Panther are...</td>\n",
       "      <td>They're the Tolkien white guys.\\n\\nEdit: Appar...</td>\n",
       "      <td>[(The, DT), (only, JJ), (two, CD), (white, JJ)...</td>\n",
       "      <td>[(They, PRP), ('re, VBP), (the, DT), (Tolkien,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "1        Calm down about the Net Neutrality thing...   \n",
       "2                                                  V   \n",
       "3  If your surprised that Jeffrey Epstein commite...   \n",
       "4  A new Navy recruit has his first day on the su...   \n",
       "5  The only two white actors in Black Panther are...   \n",
       "\n",
       "                                                body  \\\n",
       "1  Paying additional money to access certain site...   \n",
       "2  V\\n\\n*Edit: seems like the ctrl key on my keyb...   \n",
       "3  Imagine how surprised he must have been.\\n\\nEd...   \n",
       "4  He speaks with the officer, who assigns him hi...   \n",
       "5  They're the Tolkien white guys.\\n\\nEdit: Appar...   \n",
       "\n",
       "                                           title_tag  \\\n",
       "1  [(Calm, NNP), (down, RB), (about, IN), (the, D...   \n",
       "2                                          [(V, NN)]   \n",
       "3  [(If, IN), (your, PRP$), (surprised, VBD), (th...   \n",
       "4  [(A, DT), (new, JJ), (Navy, NNP), (recruit, NN...   \n",
       "5  [(The, DT), (only, JJ), (two, CD), (white, JJ)...   \n",
       "\n",
       "                                            body_tag  \n",
       "1  [(Paying, VBG), (additional, JJ), (money, NN),...  \n",
       "2  [(V, NNP), (*, NNP), (Edit, NNP), (:, :), (see...  \n",
       "3  [(Imagine, NNP), (how, WRB), (surprised, VBD),...  \n",
       "4  [(He, PRP), (speaks, VBZ), (with, IN), (the, D...  \n",
       "5  [(They, PRP), ('re, VBP), (the, DT), (Tolkien,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tagging\n",
    "joke_df[\"title_tag\"] = joke_df[\"title_t\"].apply(pos_tag)\n",
    "joke_df[\"body_tag\"] = joke_df[\"body_t\"].apply(pos_tag)\n",
    "\n",
    "select_col = [\"title\", \"body\", \"title_tag\", \"body_tag\"]\n",
    "joke_df[select_col].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-literature",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "#### Explore the ability to identify noun from NLTK package\n",
    "\n",
    "Because I am interested only in character or organisation in the jokes, so I want to find a way to get only those nouns. NLTK has named entity recognition method (`ne_chunk`)that can be used to identified person or organisation, etc. More detail about this method can be found in https://www.nltk.org/book/ch07.html .\n",
    "\n",
    "The result is that `ne_chunk` did not recognise all the person and organisation noun in the jokes. For example, an officer and recruit should be recognised as a person. NLTK also misinterpret some of the words such as Imagine, Soon and Sorry as a proper noun which might happen because of the capitalisation. Though the suggestion from alvas on Stackoverflow suggested that the text should not be put into lowercase because NER was trained on normal text (https://stackoverflow.com/a/34458164). Because of the performance, I will skip the noun identification step in this trial.\n",
    "\n",
    "As this blog https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da suggested, SpaCy may be a better option in identification than NLTK. SpaCy will be tested later.\n"
    "\n",
    "*Note: See additional note in discussion session*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "numerical-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named entity recognition test on body\n",
    "ne_list = joke_df[\"body_tag\"].apply(ne_chunk).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "collectible-bibliography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Noun that got recognised by ne_chunk==\n",
      "[Tree('GPE', [('Imagine', 'NNP')]), Tree('ORGANIZATION', [('Tolkien', 'NNP')]), Tree('GPE', [('None', 'NN')]), Tree('PERSON', [('Trump', 'NNP')]), Tree('PERSON', [('Dad', 'NNP')]), Tree('PERSON', [('Daddy', 'NNP')]), Tree('GPE', [('American', 'JJ')]), Tree('GPE', [('Japan', 'NNP')]), Tree('PERSON', [('Soon', 'NNP')]), Tree('ORGANIZATION', [('Christmas', 'NNP')]), Tree('PERSON', [('Pete', 'NNP')]), Tree('GPE', [('Canada', 'NNP')]), Tree('PERSON', [('Sorry', 'NNP')]), Tree('GPE', [('Bitcoin', 'NNP')]), Tree('GPE', [('College', 'NN')]), Tree('GPE', [('Good', 'JJ')]), Tree('GPE', [('Please', 'NNP')]), Tree('GPE', [('Midlife', 'NNP')]), Tree('ORGANIZATION', [('Muslim', 'NNP')]), Tree('GPE', [('United', 'NNP')])]\n",
      "\n",
      "==Noun that did not get recognised==\n",
      "[('money', 'NN'), ('access', 'NN'), ('sites', 'NNS'), ('sense', 'NN'), ('pride', 'NN'), ('accomplishment', 'NN'), ('V', 'NNP'), ('*', 'NNP'), ('Edit', 'NNP'), ('ctrl', 'NN'), ('key', 'NN'), ('keyboard', 'NN'), ('Edit', 'NN'), ('Thank', 'NN'), ('Redditors', 'NNPS'), ('post', 'NN'), ('everybody', 'NN'), ('sorry', 'NN'), ('re', 'NNS'), ('officer', 'NN'), ('post', 'NN'), ('Go', 'NNP'), ('stand', 'NN'), ('periscope', 'NN'), ('personnel', 'NNS'), ('periscope', 'NN'), ('recruit', 'NN'), ('orders', 'NNS'), ('stands', 'NNS'), ('periscope', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Get noun\n",
    "entity_reg = []\n",
    "not_reg = []\n",
    "for sentence in ne_list:\n",
    "    for i in sentence:\n",
    "        if len(i) == 1:\n",
    "            entity_reg.append(i)\n",
    "        elif i[1][0] == \"N\":\n",
    "            not_reg.append(i)\n",
    "            \n",
    "print(\"==Noun that got recognised by ne_chunk==\")\n",
    "print(entity_reg[:20])\n",
    "print(\"\\n==Noun that did not get recognised==\")\n",
    "print(not_reg[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-gregory",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### 3. Get the noun \n",
    "\n",
    "A noun can be identified from the tag in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "western-edition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_tag</th>\n",
       "      <th>body_tag</th>\n",
       "      <th>joke_noun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(Calm, NNP), (down, RB), (about, IN), (the, D...</td>\n",
       "      <td>[(Paying, VBG), (additional, JJ), (money, NN),...</td>\n",
       "      <td>[Calm, Neutrality, thing, money, access, sites...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(V, NN)]</td>\n",
       "      <td>[(V, NNP), (*, NNP), (Edit, NNP), (:, :), (see...</td>\n",
       "      <td>[Edit, ctrl, key, keyboard]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(If, IN), (your, PRP$), (surprised, VBD), (th...</td>\n",
       "      <td>[(Imagine, NNP), (how, WRB), (surprised, VBD),...</td>\n",
       "      <td>[Jeffrey, Epstein, suicide, morning, Imagine, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(A, DT), (new, JJ), (Navy, NNP), (recruit, NN...</td>\n",
       "      <td>[(He, PRP), (speaks, VBZ), (with, IN), (the, D...</td>\n",
       "      <td>[Navy, recruit, day, submarine, officer, post,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[(The, DT), (only, JJ), (two, CD), (white, JJ)...</td>\n",
       "      <td>[(They, PRP), ('re, VBP), (the, DT), (Tolkien,...</td>\n",
       "      <td>[actors, Black, Panther, Martin, Freeman, Bilb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           title_tag  \\\n",
       "1  [(Calm, NNP), (down, RB), (about, IN), (the, D...   \n",
       "2                                          [(V, NN)]   \n",
       "3  [(If, IN), (your, PRP$), (surprised, VBD), (th...   \n",
       "4  [(A, DT), (new, JJ), (Navy, NNP), (recruit, NN...   \n",
       "5  [(The, DT), (only, JJ), (two, CD), (white, JJ)...   \n",
       "\n",
       "                                            body_tag  \\\n",
       "1  [(Paying, VBG), (additional, JJ), (money, NN),...   \n",
       "2  [(V, NNP), (*, NNP), (Edit, NNP), (:, :), (see...   \n",
       "3  [(Imagine, NNP), (how, WRB), (surprised, VBD),...   \n",
       "4  [(He, PRP), (speaks, VBZ), (with, IN), (the, D...   \n",
       "5  [(They, PRP), ('re, VBP), (the, DT), (Tolkien,...   \n",
       "\n",
       "                                           joke_noun  \n",
       "1  [Calm, Neutrality, thing, money, access, sites...  \n",
       "2                        [Edit, ctrl, key, keyboard]  \n",
       "3  [Jeffrey, Epstein, suicide, morning, Imagine, ...  \n",
       "4  [Navy, recruit, day, submarine, officer, post,...  \n",
       "5  [actors, Black, Panther, Martin, Freeman, Bilb...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract only noun\n",
    "def find_noun(sentence):\n",
    "    noun_list = []\n",
    "    for i in sentence:\n",
    "        if (i[1][0] == \"N\") and (len(i[0]) > 1):\n",
    "            noun_list.append(i[0])\n",
    "    return noun_list\n",
    "\n",
    "# Find noun\n",
    "joke_df[\"title_noun\"] = joke_df[\"title_tag\"].apply(find_noun)\n",
    "joke_df[\"body_noun\"] = joke_df[\"body_tag\"].apply(find_noun)\n",
    "\n",
    "# Join title and body\n",
    "joke_df[\"joke_noun\"] = joke_df[\"title_noun\"] + joke_df[\"body_noun\"]\n",
    "\n",
    "select_col = [\"title_tag\", \"body_tag\", \"joke_noun\"]\n",
    "joke_df[select_col].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-leone",
   "metadata": {},
   "source": [
    "### 4. Lemmatisation\n",
    "\n",
    "As showed below, capitalise has an effect on lemmatisation. So it needs to convert to lowercase before further process.\n",
    "Moreover, because the goal is to find the most mentioned person and organisation in the jokes, only need to count the unique noun. This make longer joke have less effect to the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "shared-beach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Girls\n",
      "girl\n"
     ]
    }
   ],
   "source": [
    "# The effect of case to lemmatisation\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "print(wnl.lemmatize(\"Girls\"))\n",
    "print(wnl.lemmatize(\"girls\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "modified-counter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    [accomplishment, sense, site, access, pride, m...\n",
       "2                          [edit, keyboard, ctrl, key]\n",
       "3    [sorry, re, suicide, post, edit, jeffrey, redd...\n",
       "4    [obeys, mess, stand, listen, room, hey, son, s...\n",
       "5    [actor, somebody, black, guy, tolkien, freeman...\n",
       "Name: lemma_noun, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function for lemmatisation\n",
    "def lemma(noun_list):\n",
    "    lemma = []\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for i in noun_list:\n",
    "        lemma.append(wnl.lemmatize(i.lower()))\n",
    "    return list(set(lemma))  # return only unique noun in a joke\n",
    "\n",
    "joke_df[\"lemma_noun\"] = joke_df[\"joke_noun\"].apply(lemma)\n",
    "joke_df[\"lemma_noun\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-reply",
   "metadata": {},
   "source": [
    "### 5. Separate between SFW and NSFW jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "short-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_df[joke_df[\"over_18\"]==True]\n",
    "\n",
    "# Get the list of all noun of SFW and NSFW\n",
    "sfw_df = joke_df[joke_df[\"over_18\"]==False]\n",
    "sfw_noun_list = sfw_df.loc[:][\"lemma_noun\"].to_list()\n",
    "sfw_noun_list  = [i for sub in sfw_noun_list  for i in sub]\n",
    "\n",
    "nsfw_df = joke_df[joke_df[\"over_18\"]==True]\n",
    "nsfw_noun_list = nsfw_df.loc[:][\"lemma_noun\"].to_list()\n",
    "nsfw_noun_list  = [i for sub in nsfw_noun_list  for i in sub]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-option",
   "metadata": {},
   "source": [
    "### 6. Count the nouns\n",
    "\n",
    "Though the comparison of the count should be in percentage, because I did not select only the noun that is person and organisation, showing percentage can lead to misinterpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "scenic-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count noun\n",
    "def count_noun(noun_list):\n",
    "    noun_dict = {}\n",
    "    for i in noun_list:\n",
    "        if i in noun_dict:\n",
    "            noun_dict[i] += 1\n",
    "        else:\n",
    "            noun_dict[i] = 1\n",
    "    return noun_dict\n",
    "\n",
    "# SFW count\n",
    "sfw_noun_dict = count_noun(sfw_noun_list)\n",
    "sfw_noun_count = pd.DataFrame.from_dict(sfw_noun_dict,  orient='index').reset_index()\n",
    "sfw_noun_count.columns = [\"noun_from_sfw\", \"count\"]\n",
    "sfw_noun_count = sfw_noun_count.sort_values(by=[\"count\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "# NSFW count\n",
    "nsfw_noun_dict = count_noun(nsfw_noun_list)\n",
    "nsfw_noun_count = pd.DataFrame.from_dict(nsfw_noun_dict,  orient='index').reset_index()\n",
    "nsfw_noun_count.columns = [\"noun_from_nsfw\", \"count\"]\n",
    "nsfw_noun_count = nsfw_noun_count.sort_values(by=[\"count\"], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "perfect-boulder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun_from_sfw</th>\n",
       "      <th>count</th>\n",
       "      <th>noun_from_nsfw</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>man</td>\n",
       "      <td>101</td>\n",
       "      <td>sex</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>edit</td>\n",
       "      <td>86</td>\n",
       "      <td>man</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>time</td>\n",
       "      <td>85</td>\n",
       "      <td>wife</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wife</td>\n",
       "      <td>83</td>\n",
       "      <td>day</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>day</td>\n",
       "      <td>82</td>\n",
       "      <td>woman</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>year</td>\n",
       "      <td>77</td>\n",
       "      <td>time</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>people</td>\n",
       "      <td>74</td>\n",
       "      <td>guy</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>guy</td>\n",
       "      <td>63</td>\n",
       "      <td>door</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>way</td>\n",
       "      <td>57</td>\n",
       "      <td>edit</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>joke</td>\n",
       "      <td>57</td>\n",
       "      <td>night</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>woman</td>\n",
       "      <td>50</td>\n",
       "      <td>girl</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>friend</td>\n",
       "      <td>48</td>\n",
       "      <td>hand</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>son</td>\n",
       "      <td>48</td>\n",
       "      <td>year</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>girl</td>\n",
       "      <td>47</td>\n",
       "      <td>father</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>thing</td>\n",
       "      <td>46</td>\n",
       "      <td>home</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>trump</td>\n",
       "      <td>44</td>\n",
       "      <td>house</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>house</td>\n",
       "      <td>44</td>\n",
       "      <td>way</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>well</td>\n",
       "      <td>41</td>\n",
       "      <td>joke</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>thanks</td>\n",
       "      <td>41</td>\n",
       "      <td>one</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>home</td>\n",
       "      <td>40</td>\n",
       "      <td>room</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   noun_from_sfw  count noun_from_nsfw  count\n",
       "0            man    101            sex   42.0\n",
       "1           edit     86            man   40.0\n",
       "2           time     85           wife   40.0\n",
       "3           wife     83            day   34.0\n",
       "4            day     82          woman   29.0\n",
       "5           year     77           time   23.0\n",
       "6         people     74            guy   22.0\n",
       "7            guy     63           door   21.0\n",
       "8            way     57           edit   21.0\n",
       "9           joke     57          night   20.0\n",
       "10         woman     50           girl   19.0\n",
       "11        friend     48           hand   19.0\n",
       "12           son     48           year   17.0\n",
       "13          girl     47         father   17.0\n",
       "14         thing     46           home   17.0\n",
       "15         trump     44          house   17.0\n",
       "16         house     44            way   16.0\n",
       "17          well     41           joke   16.0\n",
       "18        thanks     41            one   15.0\n",
       "19          home     40           room   15.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare noun from SFW and NSFW jokes\n",
    "pd.concat([sfw_noun_count, nsfw_noun_count], axis=1)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-annual",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Few issues occur in this trial. First, `ne_chunk` did not recognise all of the person and organisation and few words got the wrong tag. There might be a way to resolve this issue from `ne_chunk`, but SpaCy package is also interesting to test.\n",
    "Another issue is it is possible that the title and some part of the body are not a part of the joke. The example is shown below.",
    "\n",
    "*Additional note: After I do more research about name recognition, it may recognise only specific person name, not an occupation or a role. Thus, this may require another solution.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "administrative-village",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Title==\n",
      "This is the dirty joke my 85yo grandad told to our whole family by memory\n",
      "\n",
      "==Body==\n",
      "A male whale and a female whale were swimming off the coast of Japan when they noticed a whaling ship.  The male whale recognized it as the same ship that had harpooned his father many years earlier. He said to the female whale, \"Lets both swim under the ship and blow out of our air holes at the same time and it should cause the ship to turn over and sink.\" They tried it and sure enough, the ship turned over and quickly sank. \n",
      "\n",
      "Soon however, the whales realized the sailors had jumped overboard and were swimming to the safety of shore. The male was enraged that they were going to get away and told the female, \"Let's swim after them and gobble them up before they reach the shore.\" At this point, he realized the female was becoming reluctant to follow him. \"Look,\" she said, \"I went along with the blow job, but I absolutely refuse to swallow the seamen.\" \n",
      "\n",
      "Edit: I think it's bad that I'm more excited watching this get ups that I was about the whole of Christmas \n"
     ]
    }
   ],
   "source": [
    "print(\"==Title==\\n\" + joke_df.loc[18][\"title\"])\n",
    "print(\"\\n==Body==\\n\" + joke_df.loc[18][\"body\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-undergraduate",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.wordnet\n",
    "\n",
    "https://www.nltk.org/book/ch07.html\n",
    "\n",
    "https://stackoverflow.com/a/34458164\n",
    "\n",
    "https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
